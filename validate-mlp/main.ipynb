{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sparse_models import SparseMLP, SimpleSparseMLP\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from interp_utils import register_hook, remove_hooks\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "sparsity_levels = torch.arange(-5,5)\n",
    "sparsity = 3\n",
    "fname = f'mlp_F20000_S{sparsity}_R2.pt'\n",
    "\n",
    "# mlp = SparseMLP(n_features=6000, d_model=768, disable_comet=True)\n",
    "# mlp.load_state_dict(torch.load(f'../sparse-mlps/{fname}'))\n",
    "\n",
    "mlp = SimpleSparseMLP(n_features=20000, d_model=768)\n",
    "mlp.load_state_dict(torch.load(f'../../ts-autoencoder/mlps/{fname}'))\n",
    "\n",
    "mlp.to(device)\n",
    "\n",
    "def mlp_replacement_hook(module, inp, out):\n",
    "    with torch.no_grad():\n",
    "        acts =  mlp.get_acts(inp[0])\n",
    "        pred = mlp.decoder(acts) + mlp.output_bias[None]\n",
    "        return pred\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roneneldan/TinyStories-33M')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing hook:  GPTNeoMLP mlp_replacement_hook\n"
     ]
    }
   ],
   "source": [
    "remove_hooks(model)\n",
    "batch_toks = tokenizer.encode(' Once upon a time', return_tensors='pt').to(device)\n",
    "register_hook(model.transformer.h[0].mlp, mlp_replacement_hook)\n",
    "with torch.no_grad():\n",
    "    x = model(batch_toks)\n",
    "    remove_hooks(model)\n",
    "    y = model(batch_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a red cat named Fluffy. Fluffy loved to eat milk. One day, Fluffy�! She wanted to play with a crystal.\n",
      "\n",
      "\n",
      "Maggie asked everyone, \"Will you play with me?\" But no one said yes.\n",
      "\n",
      "\n",
      "So Flappy didn't give up. She thought and thought, and then she decided to try and steal the red crystal. She crept up to the door and called, \"No, don't do it!\"\n",
      "\n",
      "\n",
      "The voice said, \"Why not? I'm scared and didn't want to play with you.\"\n",
      "\n",
      "\n",
      "Maggie thought for a moment. Then she said, \"It's ok Fluffy. I don't want you to have fun with me.\"\n",
      "\n",
      "\n",
      "The voice said, \"Thank you for being kind. I'll leave soon. Just be careful when you're eating.\"\n",
      "\n",
      "\n",
      "Fluffy was embarrassed. Did she know that she could eat the red crystal.\n",
      "\n",
      "\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "Max said, \"Can I have a cookie?\"\n",
      "\n",
      "\n",
      "Max replied, \"No, it's too expensive.\"\n",
      "Max felt sad. He thought it was too silly. He thought, \"Yeah! I'm going to play here anyway!\"\n",
      "\n",
      "\n",
      "Max walked around the park, with Max running and laughing.\n",
      "\n",
      "\n",
      "Suddenly he spotted something. It was a tiny, cute cute baby. Max smiled and said, �, \"Doalloween boys\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.distributions as dists\n",
    "def enc(s):\n",
    "    return tokenizer.encode(s, return_tensors='pt').to(device)\n",
    "\n",
    "def dec(tok_ids):\n",
    "    return tokenizer.batch_decode(tok_ids)\n",
    "remove_hooks(model)\n",
    "\n",
    "register_hook(model.transformer.h[0].mlp, mlp_replacement_hook)\n",
    "\n",
    "prompt = 'Once upon a time'\n",
    "# for _ in range(20):\n",
    "#     prompt = dec(model.generate(enc(prompt)), temperature=1.0)[0]\n",
    "# print(prompt)\n",
    "\n",
    "for _ in range(300):\n",
    "    logits = model(enc(prompt)).logits\n",
    "\n",
    "    sampled_tok = tokenizer.decode(dists.Categorical(logits=(1/0.9)*logits[0,-1]).sample())\n",
    "\n",
    "    prompt += sampled_tok\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
